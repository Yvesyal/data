# 分布式架构面试 


## 分布式技术体系 
 SpringCloud分布式架构组件 
  Eureka 
  
* 服务注册中心

> 与客户端一起通过心跳机制完成服务注册与发现
通过搭建集群完成故障转移达到高可用性的目的，因为去中心化，满足了CAP定理中的AP原则（侧重可用性）

 *  Ribbon 
  
客户端负载均衡框架，支持负载均衡算法如下：

轮询策略（默认）

加权轮询：

> 原理：一开始为轮询策略，并开启一个计时器，每30秒收集一次每个provider的平均响应时间，当信息足够时，给每个provider附上一个权重，并按权重随机选择

随机策略

> 最少并发数策略：选择正在请求中的并发数最小的provider，除非这个provider在熔断中。

*   Feign 
  
> 只需要定义服务绑定接口且以注解声明的方法，优雅而简单的实现了服务调用
本身集成了Ribbon，支持负载均衡算法

*   Hystrix 
  
> 断路器组件，功能：资源隔离、限流、熔断、降级、运维监控

* 实现原理：

> 当对某个服务的调用在一定的时间内（默认10s），有超过一定次数（默认20次）并且失败率超过一定值（默认50%），该服务的断路器会打开，返回一个由开发者设定的fallback
然后等待一段时间（默认5S），断路器进入半开状态，此时服务如果恢复正常调用，那么断路器关闭；否则继续打开，继续等待5S进入半开状态。。。。

* 资源隔离解释：
>有一个分布式系统，服务A依赖于服务B，服务B依赖于服务C/D/E。在这样一个成熟的系统内，比如说最多可能只有100个线程资源。正常情况下,40个线程并发调用服务C，各30个线程并发调用 D/E。 
调用服务 C，只需要 20ms，现在因为服务C故障了，比如延迟，或者挂了，此时线程会吊住2s左右。40个线程全部被卡住，由于请求不断涌入，其它的线程也用来调用服务 C，同样也会被卡住。这样导致服务B的线程资源被耗尽，无法接收新的请求，甚至可能因为大量线程不断的运转，导致自己宕机。服务A也挂了。
Hystrix可以对其进行资源隔离，比如限制服务B只有40个线程调用服务C。当此40个线程被hang住时，其它60个线程依然能正常调用工作。从而确保整个系统不会被拖垮


 *  Zuul/Gateway 
> 服务网关组件，具有对请求的路由和过滤两大功能
Zuul大部分功能都是通过过滤器来实现的。Zuul中定义了四种标准过滤器类型，这些过滤器类型对应于请求的典型生命周期。

>> (1) PRE：这种过滤器在请求被路由之前调用。我们可利用这种过滤器实现身份验证、在集群中选择请求的微服务、记录调试信息等。

>> (2) ROUTING：这种过滤器将请求路由到微服务。这种过滤器用于构建发送给微服务的请求，并使用Apache HttpClient或Netfilx Ribbon请求微服务。

>> (3) POST：这种过滤器在路由到微服务以后执行。这种过滤器可用来为响应添加标准的HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等。

>> (4) ERROR：在其他阶段发生错误时执行该过滤器。

  Config 
> 分布式配置中心，保证整个项目的配置文件一致性

* 优点：
> 修改配置无需重新部署微服务
便于统一管理，提供了profile属性，可以统一切换配置属性文件

* spring cloud config 配置更新有两种方式：
1.配置git仓库的web hook，当git仓库有更新时自动调用bus提供的刷新接口，刷新缓存；（自动）

2.手工调用bus提供的刷新接口（手动）


参考：
https://blog.csdn.net/hanruikai/article/details/82587492

  Bus 
* 消息服务总线
> 将分布式系统的节点与轻量级消息代理链接，可以实现广播状态更改（例如配置更改）或广播其他管理指令，用作应用程序之间的通信通道

Spring Cloud Bus支持RabbitMQ和Kafka

RabbitMQ + Web Hook + Spring Cloud Bus完成远程配置文件统一自动更新

参考：
https://www.cnblogs.com/rmxd/p/11586667.html
  Sleuth 

* 微服务调用链跟踪

微服务跟踪(sleuth)其实是一个工具,它在整个分布式系统中能跟踪一个用户请求的过程(包括数据采集，数据传输，数据存储，数据分析，数据可视化)，捕获这些跟踪数据，就能构建微服务的整个调用链的视图，这是调试和监控微服务的关键工具。

参考：
https://www.cnblogs.com/dengpengbo/p/11109254.html

## SpringCloud For Alibaba 
 *  Nacos 
Nacos 支持基于 DNS 和基于 RPC 的服务发现、动态配置服务（可以做配置中心）、动态 DNS 服务
与eureka，zookeeper功能非常类似，但是更优秀：
> 使用简单，不需要另外单独部署注册中心工程，直接启动jar即可
性能更好，并且支持AP,CP切换
配置中心，相当于eureka+spring cloud config
控制台相对eureka更完美，更直观

阿里nacos异常情况 leader挂了

1.不影响服务之间互相调用

2.不影响服务注册

3.不影响服务正常启动拉取配置文件

4.选举新leader差不多4,5秒钟


入门参考：
https://nacos.io/zh-cn/docs/quick-start.html
 ## Sentinel服务熔断与限流 
跟Hystrix相似，但是更加强大

单独的一个组件，直接独立出来，可以直接控制台界面细粒度的统一配置：流控，速控，服务熔断与降级隔离等

参考：
https://www.cnblogs.com/leeego-123/p/12706774.html
  Seata 
## 分布式事务中间件

前身是阿里巴巴Fescar，支持AT,TCC,Saga模式

AT 模式： 一种**业务无侵入**的分布式事务解决方案。在 AT 模式下，用户只需关注自己的“业务 SQL”，用户的 “业务 SQL” 作为一阶段，Seata 框架会自动生成事务的**二阶段**提交和回滚操作 

​TCC 模式： 一种业务侵入式的分布式事务解决方案，TCC 模式需要用户根据自己的业务场景实现 Try、Confirm 和 Cancel 三个操作；事务发起方在一阶段执行 Try 方式，在二阶段提交执行 Confirm 方法，二阶段回滚执行 Cancel 方法 

​Saga 模式： 分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作 

三个重要组件组成：
Transaction Coordinator(TC)：管理全局的分支事务的状态，用于全局性事务的提交和回滚。
Transaction Manager(TM)：事务管理器，用于开启全局事务、提交或者回滚全局事务，是全局事务的开启者。
Resource Manager(RM)：资源管理器，用于分支事务上的资源管理，向TC注册分支事务，上报分支事务的状态，接受TC的命令来提交或者回滚分支事务。


##  Dubbo&Zookeeper 
  Dubbo 
   服务注册与调度 
Dubbo 框架是用来处理分布式系统中，服务发现与注册以及调用问题的，并且管理调用过程

推荐使用协议：dubbo
工作流程：
 

使用的通信框架：Netty 
关闭启动依赖服务检查：check="false" 
   负载均衡策略 
支持的负载均衡策略：


   容错机制 
Dubbo一共提供了六种容错机制，可以在Client端调用的时候进行设置

Failover Cluster（缺省）

失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries="2" 来设置重试次数（不含第一次，缺省为2次）。

Failfast Cluster

快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。

Failsafe Cluster

失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。

Failback Cluster

失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。

Forking Cluster

并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks="2" 来设置最大并行数。

Broadcast Cluster

广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。

一般查询操作会使用Failover，事务请求会使用Failfast。

  Zookeeper 
   服务集群，迁移（扩容） 
zookeeper通过搭建服务注册中心集群，实现高可用，但是需要注意的是：
zookeeper符合CAP定理中的CP原则（一致性），集群服务中心化，需要选举leader才能提供服务，所以在选举的时候服务是不可用的

zookeeper的选举规则为：过半原则
zookeeper集群过半失效也不可用

选举PK原则
先比较Zxid,谁大谁当Leader，如果Zxid比较不出来，再比较选举ID，谁大谁当Leader(前提是要满足过半机制)

迁移扩容参考：
https://blog.csdn.net/jiabeis/article/details/100542405

核心六个字：
先扩容，再缩容

  ##  分布式锁 
公平独占锁：每个对象都有一个独占的锁，大家一次排队出列
可重入锁：每一组对象中只要一个对象拥有锁，那么整组对象都可以重复使用（一人得道鸡犬升天）

zookeeper分布式锁：高可靠（节点创建都是有集群leader完成，并同步到follower）
redis分布式锁：高并发

   Watcher机制分析 
ZooKeeper 提供了分布式数据的发布/订阅功能。

在 ZooKeeper 中，引入了 Watcher 机制来实现这种分布式的通知功能。
ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，
当服务器的一些特定事件触发了这个 Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能

流程：
客户端在向 ZooKeeper 服务器注册 Watcher 的同时，会将 Watcher 对象存储在客户端的WatchManager 中。
当ZooKeeper 服务器触发 Watcher 事件后，会向客户端发送通知，
客户端线程从 WatchManager 的实现类中取出对应的 Watcher 对象来执行回调逻辑。
   动态感知服务器上下线 
主要是利用Watch机制来监听服务器启动时在zookeeper服务器上短暂节点（业务服务器信息）的创建和删除

1.感知上线
  当服务器启动的时候通过程序知道后会同时在zookeeper的servers节点下创建一个新的短暂有序节点来存储当前服务器的信息。客户端通过对servers节点的watch可以立马知道有新的服务器上线了

2.感知下线
  当我们有个服务器下线后，对应的servers下的短暂有序节点会被删除，此时watch servers节点的客户端也能立马知道哪个服务器下线了，能够及时将访问列表中对应的服务器信息移除，从而实现及时感知服务器的变化。

参考：
https://www.cnblogs.com/dengpengbo/p/10443547.html
 ##   选举机制原理 
1、Serverid：服务器ID
比如有三台服务器，编号分别是1,2,3。
编号越大在选择算法中的权重越大。

2、Zxid：数据ID
服务器中存放的最大数据ID.
值越大说明数据越新，在选举算法中数据越新权重越大。


## 选举流程简述
目前有5台服务器，每台服务器均没有数据，它们的编号分别是1,2,3,4,5,按编号依次启动，它们的选择举过程如下：

1、服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking(选举状态)。
2、服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。
3、服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为领导者，服务器1,2成为小弟。
4、服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为小弟。
5、服务器5启动，后面的逻辑同服务器4成为小弟。
##  ElasticSearch 
  为什么使用es 
举例说明：
​ 因为在我们商城中的数据，将来会非常多，所以采用以往的模糊查询，模糊查询前置配置，会放弃索引，导致商品查询是全表扫面，在百万级别的数据库中，效率非常低下，而我们使用ES做一个全文索引，我们将经常查询的商品的某些字段，比如说商品名，描述、价格还有id这些字段我们放入我们索引库里，可以提高查询速度。

  集群、节点、索引、文档、类型概念 
集群(cluster):由一个或多个节点组成, 并通过集群名称与其他集群进行区分

节点(node):单个ElasticSearch实例. 通常一个节点运行在一个隔离的容器或虚拟机中

索引(index):在ES中, 索引是一组文档的集合（就是我们所说的一个日志），相当于关系型数据库的“库”

文档：相当于关系型数据库中的行数据

类型：相当于关系型数据中的表
  什么是分片 
分片(shard):因为ES是个分布式的搜索引擎, 所以索引通常都会分解成不同部分, 而这些分布在不同节点的数据就是分片. ES自动管理和组织分片, 并在必要的时候对分片数据进行再平衡分配, 所以用户基本上不用担心分片的处理细节，一个分片默认最大文档数量是20亿.

分片查询类型：
randomizeacross shards
随机选择分片查询数据，es的默认方式

_local
优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有IO问题但有可能造成负载不均问题。数据量是完整的。

_primary
只在主分片中查询不去副本查，一般数据完整。

_primary_first
优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。

_only_node
只在指定id的节点中的分片中查询，数据可能不完整。

_prefer_node
优先在指定你给的节点中查询，一般数据完整。

_shards
在指定分片中查询，数据可能不完整。

_only_nodes
可以自定义去指定的多个节点查询，es不提供此方式需要改源码。
  实现Master选举过程 
前置前提：
（1）只有候选主节点（master：true）的节点才能成为主节点。

（2）最小主节点数（min_master_nodes）的目的是防止脑裂。

选举流程大致描述如下：

第一步：确认候选主节点数达标，elasticsearch.yml 设置的值
discovery.zen.minimum_master_nodes；

第二步：比较：先判定是否具备 master 资格，具备候选主节点资格的优先返回；
若两节点都为候选主节点，则 id 值小的为主节点。注意这里的 id 为 string 类型。
  索引文档过程 


参考： 

https://developer.51cto.com/art/201904/594615.htm 
 ##  搜索的过程 
核心就是通过分词器建立倒排索引

参考：
https://www.cnblogs.com/unnunique/p/9376678.html
##   更新和删除文档过程 
参考：
https://www.cnblogs.com/unnunique/p/9376678.html
 ##  大数据聚合 
execution_hint 参数选择

参考：
https://blog.csdn.net/qq_24636385/article/details/101062908
 ##  高并发读写一致 
ES 数据并发冲突控制是基于的乐观锁和版本号的机制

一个document第一次创建的时候，它的_version内部版本号就是1；以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1；哪怕是删除，也会对这条数据的版本号加1(假删除)。

客户端对es数据做更新的时候，如果带上了版本号，那带的版本号与es中文档的版本号一致才能修改成功，否则抛出异常。如果客户端没有带上版本号，首先会读取最新版本号才做更新尝试，这个尝试类似于CAS操作，可能需要尝试很多次才能成功。乐观锁的好处是不需要互斥锁的参与。

es节点更新之后会向副本节点同步更新数据(同步写入)，直到所有副本都更新了才返回成功

 ##  ELK三者职责及原理 
E：ElasticSearch 接收logstash文件数据，通过分词创建索引文件数据
L: Logstash 通过过滤获取日志文件数据，发送给es
K:Kibana 通过索引查询es中的文件数据分析并展示
##  Redis 
  数据结构类型 
string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)
  主从复制过程原理 
  
1、保存主节点信息

2、建立socket连接

3、发送ping命令

4、权限验证

5、 同步数据集 

6、维持心跳检测

7、命令持续复制

 ##  管道模式Pipeline 
Redis 的 pipeline(管道)功能在命令行中没有，但 redis 是支持 pipeline 的，而且在各个语言版的 client 中都有相应的实现。 由于网络开销延迟，就算 redis server 端有很强的处理能力，也会由于收到的 client 消息少，而造成吞吐量小。当 client 使用 pipelining 发送命令时，redis server 必须将部分请求放到队列中（使用内存），执行完毕后一次性发送结果！

Pipeline 在某些场景下非常有用，比如有多个 command 需要被“及时的”提交，而不需要“及时的”响应，那么 pipeline 就可以充当这种“批处理”的工具；而且在一定程度上，可以较大的提升性能，性能提升的原因主要是 TCP 连接中减少了“交互往返”的时间
  缓存&数据库双写一致性解决方案 
延时双删策略
异步更新缓存(基于订阅binlog的同步机制)

 ##  RDB，AOF持久化策略 
RDB持久化是将某个时间点上Redis中的数据保存到一个RDB文件中，也叫快照持久化；
RDB优点：

​ RDB文件小，非常适合定时备份，用于灾难恢复。

​ 因为RDB文件中直接存储的是内存数据，而AOF文件中存储的是一条条命令，需要应用命令。Redis加载RDB文件的速度比AOF快很多。

缺点：

RDB持久化方式不能做到实时/秒级持久化。实时持久化要全量刷内存到磁盘，成本太高。每秒fork子进程也会阻塞主进程，影响性能。RDB文件是二进制文件，随着Redis不断迭代有多个rdb文件的版本，不支持跨版本兼容。老的Redis无法识别新的RDB文件格式。 

AOF持久化是通过保存Redis服务器所执行的写**命令**来记录数据库数据的 ，图示

AOF优缺点
​ 优点：数据的完整性和一致性更高
​ 缺点：因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢，因为不管怎么压缩，它的命令基数总是在不断增长的。

​ RDB和AOF模式一般都会一起使用，毕竟数据的稳定性高于一切，两者同时持久化时，数据的恢复流程图如下：
  哨兵Sentinel集群部署及优劣 
优点：在主从复制的基础上实现了自动选举过程，当主节点挂掉后，从节点会进行选举，产生新的主节点，继续提供写操作，满足CAP定理中的CP（一致性，分区容错）

缺点：因为选举过程中是写操作不可用的，所以并不是完美的高可用
  Redis Cluster集群部署及优劣 
优点：通过hash槽算法，将数据分布在不同的节点，然后每个节点会有一个从节点进行备份，实现了去中心化，任何节点都可以进行写操作，满足CAP定理中的AP原则（可用性，分区容错）

缺点：因为每个节点都可以进行写操作，高并发操作下可能会出现数据的不一致性

  缓存雪崩，穿透，击穿概念及解决方案 

缓存穿透：缓存不存在，数据库也不存在，用户恶意频繁访问，导致每次访问都进入到DB层，服务器压力过大崩溃----》将用户恶意攻击的查询数据缓存到redis，保存为null

缓存击穿：某条热门数据过期，访问一拥而入，导致单点击穿，DB层服务崩溃----》设置为永不过期

缓存雪崩：大片数据在同一时间或者相隔很短时间过期，DB层访问暴增服务崩溃---》各数据设置过期时间不尽相同
##  MQ 
  为什么使用MQ 
主要是：解耦、异步、削峰。
  MQ中间件对比 

 消息模式类型 
rabbitmq：

1.基本消息模型
2. work消息模型
3. 订阅模型分类
4. 订阅模型-Fanout
5. 订阅模型-Direct
6. 订阅模型-Topic
  幂等性保证 
全局唯一ID + Redis

生产者在发送消息时，为每条消息设置一个全局唯一的messageId，消费者拿到消息后，使用setnx命令，将messageId作为key放到redis中：setnx(messageId,1)，若返回1，说明之前没有消费过，正常消费；若返回0，说明这条消息之前已消费过，抛弃。

※ setnx命令，若给定的key不存在，执行set操作，返回1，若给定的Key已存在，不做任何操作，返回0。

  高可用保证 
普通集群模式:
一个简单的集群,并没有考虑高可用. 并且性能开销巨大.容易造成单实例的性能瓶颈. 并且如果真正有数据的那个queue的实例宕机了. 那么其他的实例就无法进行数据的拉取.
这种方式只是通过集群部署的方式提高了消息的吞吐量,但是并没有考虑到高可用.

镜像集群模式:
这种模式才是高可用模式. 与普通集群模式的主要区别在于. 无论queue的元数据还是queue中的消息都会同时存在与多个实例上.

参考：
https://blog.csdn.net/weixin_42942532/article/details/89073534
  消息可靠传输 
1）业务数据落库，落库完毕再发送消息，此处发送2条消息；
2）MQ broker的消息到下游服务；
3）confirm消息：下游服务消费后，重新生成一条消息，投递给MQ broker；
4）此时callback service监听下游服务生成的消息，对收到的消息进行持久化，保存到DB中；
5）callback service监听发送方的第二条消息，并去数据库中进行查询，有即处理好了，没有即处理异常；
6）第5）步异常后，进入该步骤。带着相关业务信息通知发送方重新发送该消息。

在第1）步中，第一条消息是发送给下游业务方的，为立即发送；第二条消息延迟5分钟后进行发送，是发送给callback service来作为检查条件，查看下游服务是否处理完毕该消息

  消息顺序保证 
RabbitMQ保证消息的顺序性，就是拆分多个 queue，每个 queue 对应一个 consumer（消费者），就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。

总之一句话：一个队列对应一个消费者
  消息延时及过期问题 
RabbitMQ提供了一种qos(服务质量保证)功能，即在非自动确认消息的前提下(设置autoAck为false)，若一定数量的消息(通过基于consume或channel设置Qos的值)未被确认前，不消费新的消息
  消息队列积压问题 
如果问题已发生：
一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下：

1）先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉；

2）新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量；

3）然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue；

4）接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据；

5）这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据；

6）等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息；

问题发生原因：
1、采用的消息模式是get模式，而不是高效的deliver模式

2、消息的生产和消费共用mq链接和channel

3、不同队列共用链接和channel，会出现队列相互影响的问题

 ##  MySQL 
  主从复制 
  
好处：
1、通过增加从服务器来提高数据库的性能，在主服务器上执行写入和更新，在从服务器上向外提供读功能，可以动态地调整从服务器的数量，从而调整整个数据库的性能。

2、提高数据安全-因为数据已复制到从服务器，从服务器可以终止复制进程，所以，可以在从服务器上备份而不破坏主服务器相应数据

3、在主服务器上生成实时数据，而在从服务器上分析这些数据，从而提高主服务器的性能


总结就一句话：MySQL的主从复制解决了数据库的读写分离，并很好的提升了读的性能
  MySQL+KeepAlived双主高可用方案 
主要利用keepalived的心跳存活机制进行重连热备

参考
https://www.cnblogs.com/kevingrace/p/6710136.html
  高性能解决方案--分库分表 
一种思想：主要从水平和垂直两个方向进行拆分

参考
https://blog.csdn.net/azhuyangjun/article/details/86976568
 ##  MyCat实现MySQL读写分离 
参考：https://www.jianshu.com/p/67abdb6ad24d
 ##  MyCat分片策略 
常用：
一、枚举法

二、固定分片hash算法（最常用）

三、范围约定

四、求模法

五、日期列分区法

六、通配取模

参考：
https://www.cnblogs.com/756623607-zhang/p/6656022.html
  MyCat全局表，ER表，分片策略 
参考：
https://www.cnblogs.com/xibei666/p/10410951.html
 ##  Nginx 
  反向代理，负载均衡 
1. 反向代理
多个客户端给服务器发送的请求，nginx服务器接收到之后，按照一定的规则分发给了后端的业务处理服务器进行处理了。此时~请求的来源也就是客户端是明确的，但是请求具体由哪台服务器处理的并不明确了，nginx扮演的就是一个反向代理角色

2.负载均衡
请求数量按照一定的规则进行分发到不同的服务器处理的规则，就是一种均衡规则
所以~将服务器接收到的请求按照规则分发的过程，称为负载均衡

负载均衡策略：
weight轮询（默认）：接收到的请求按照顺序逐一分配到不同的后端服务器，即使在使用过程中，某一台后端服务器宕机，nginx会自动将该服务器剔除出队列，请求受理情况不会受到任何影响。 这种方式下，可以给不同的后端服务器设置一个权重值（weight），用于调整不同的服务器上请求的分配率；权重数据越大，被分配到请求的几率越大；该权重值，主要是针对实际工作环境中不同的后端服务器硬件配置进行调整的。

ip_hash：每个请求按照发起客户端的ip的hash结果进行匹配，这样的算法下一个固定ip地址的客户端总会访问到同一个后端服务器，这也在一定程度上解决了集群部署环境下session共享的问题。

fair：智能调整调度算法，动态的根据后端服务器的请求处理到响应的时间进行均衡分配，响应时间短处理效率高的服务器分配到请求的概率高，响应时间长处理效率低的服务器分配到的请求少；结合了前两者的优点的一种调度算法。但是需要注意的是nginx默认不支持fair算法，如果要使用这种调度算法，请安装upstream_fair模块

url_hash：按照访问的url的hash结果分配请求，每个请求的url会指向后端固定的某个服务器，可以在nginx作为静态服务器的情况下提高缓存效率。同样要注意nginx默认不支持这种调度算法，要使用的话需要安装nginx的hash软件包

  Nginx+KeepAlived实现高可用 
Keepalived 是一种高性能的服务器高可用或热备解决方案，Keepalived 可以用来防止服务器单点故 
障的发生，通过配合 Nginx 可以实现 web 前端服务的高可用。

参考：
https://blog.csdn.net/lijian12388806/article/details/51882333
  静态资源管理（动静分离） 
通过静态资源访问配置，减少数据库IO，直接访问渲染后的静态资源（图片，html等）
  访问控制与连接限制 
Nginx自带的模块支持对并发请求数进行限制, 还有对请求来源进行限制。可以用来防止DDOS攻击


限制主要有两种类型：

连接频率限制： limit_conn_module
请求频率限制： limit_req_module
 Docker&K8S 
  主要 对镜像，容器的理解 
Docker是一个容器化平台，它将应用程序及其所有依赖项以容器的形式打包在一起，以确保应用程序在任何环境（无论是开发环境、测试环境还是生产环境）中无缝运行。

Docker容器，将一个软件包在一个完整的文件系统中，其中包含运行所需的一切：代码、运行时、系统工具、系统库等任何可以安装在服务器上的东西。

它都将始终运行相同的程序，无论软件的环境如何。

## 什么是Docker镜像？
Docker镜像是Docker容器的源代码。换句话说，Docker镜像用于创建容器。使用build命令创建镜像，并且在使用run启动时它们将生成容器

## 什么是Docker容器？
Docker容器包括应用程序及其所有依赖项，但与其他容器共享内核，在主机操作系统的用户空间中作为独立进程运行。Docker容器不依赖于任何特定的基础架构：它们可以在任何计算机，任何基础架构和任何云中运行
经典分布式解决方案 
##  分布式锁 
  数据库分布式锁 
乐观锁(基于版本号)和悲观锁(基于排它锁)

 数据库分布式锁的缺点

需要考虑数据库的可用性和性能，考虑多机部署、数据同步、主备切换等，会比较复杂
不具备可重入的特性
没有锁失效机制
不具备阻塞锁特性

实现原理
1、在数据库中创建一个表，表中包含方法名等字段，并在方法名字段上创建唯一索引 
2、想要执行某个方法，就使用这个方法名向表中插入数据
3、成功插入则获取锁，执行完成后删除对应的行数据释放锁。

  Redis 缓存分布式锁 
setnx(key,当前时间+过期时间)和Redlock机制

redis分布式锁的优势

redis高性能
命令支持好，使用方便

redis命令
setnx：当且仅当key不存在时，set一个key为val的字符串，返回1；若key存在，则什么都不做，返回0。
expire：为key设置一个超时时间，单位为second，超过这个时间锁会自动释放，避免死锁。
delete：删除key

具体实现
1、获取锁的时候使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的value值为一个随机生成的UUID。
2、获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。
3、释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放。

  Zookeeper 分布式锁 
临时有序节点来实现的分布式锁,Curator

zookeeper分布式锁的优势
高可用
可重入
阻塞锁特性
可解决失效死锁问题

zookeeper分布式锁的缺点
需要频繁的创建和删除节点，性能上不如Redis方式

具体实现
1、ZooKeeper内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名。

2、创建一个目录mylock
3、线程A想获取锁就在mylock目录下创建临时顺序节点
4、获取mylock目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁
5、线程B获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点
6、线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是不是最小的节点，如果是则获得锁

##  分布式Session 
  存入 Cookie 中 
可以将 Session 存储到 Cookie 中，但是缺点也很明显，例如每次请求都得带着 Session ，数据存储在客户端本地，是有风险的；
  Session 同步 
服务器之间进行 Session 同步，这样可以保证每个服务器上都有全部的 Session 信息，不过当服务器数量比较多的时候，同步是会有延迟甚至同步失败；
   IP 绑定策略 
使用 Nginx （或其他复杂均衡软硬件）中的 IP 绑定策略，同一个 IP 只能在指定的同一个机器访问，但是这样做失去了负载均衡的意义，当挂掉一台服务器的时候，会影响一批用户的使用，风险很大；
  使用 Redis 存储（推荐） 
把 Session 放到 Redis 中存储，虽然架构上变得复杂，并且需要多访问一次 Redis ，但是这种方案带来的好处也是很大的：

实现了 Session 共享；
可以水平扩展（增加 Redis 服务器）；
服务器重启 Session 不丢失（不过也要注意 Session 在 Redis 中的刷新/失效机制）；
不仅可以跨服务器 Session 共享，甚至可以跨平台（例如网页端和 APP 端）。
 负载均衡 
  算法 
  
   轮询 
    所有服务器均衡获得调度机会 
    
   加权轮询 
    按权重轮询 
    
   最小连接 
    最小连接数法是根据服务器当前的连接情况进行负载均衡的，当请求到来时，会选取当前连接数最少的一台服务器来处理请求 
    
   加权最小连接 
加权最小连接调度（Weighted Least-Connection Scheduling）算法是最小连接调度的超集，各个服务器用相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例

   随机 
    无序的，随机的选择一台服务器进行调度 
    
##  分布式任务 
  分布式任务调度系统模块 
1、Web模块：用来提供任务的信息，控制任务的状态、信息展示等。

2、Server模块：负责接收web端传来的任务执行的信息，下发任务调度请求给Scheduler，会去注册中心进行注册

3、Scheduler模块：接收server端传来的调度请求，将任务进行更加细化的拆分然后下发，到注册中心进行注册，获取到可以干活的worker。

4、Worker模块：负责具体的任务执行。

5、注册中心。

  框架选型（了解） 
Quartz：Java事实上的定时任务标准。但Quartz关注点在于定时任务而非数据，并无一套根据数据处理而定制化的流程。虽然Quartz可以基于数据库实现作业的高可用，但缺少分布式并行调度的功能

TBSchedule：阿里早期开源的分布式任务调度系统。代码略陈旧，使用timer而非线程池执行任务调度。众所周知，timer在处理异常状况时是有缺陷的。而且TBSchedule作业类型较为单一，只能是获取/处理数据一种模式。还有就是文档缺失比较严重

elastic-job：当当开发的弹性分布式任务调度系统，功能丰富强大，采用zookeeper实现分布式协调，实现任务高可用以及分片，目前是版本2.15，并且可以支持云开发

Saturn：是唯品会自主研发的分布式的定时任务的调度平台，基于当当的elastic-job 版本1开发，并且可以很好的部署到docker容器上。

xxl-job: 是大众点评员工徐雪里于2015年发布的分布式任务调度平台，是一个轻量级分布式任务调度框架，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。

##  分布式事务 
  解决方案 
   两阶段提交/XA方案 
即“全票通过方案”，要求所有的事务系统必须全部准备好，才可以进行事务处理，这种方案其实是将事务问题抛给了各个数据库本身，好处是数据一致性很高，缺点是耗费性能，所以这种方案一般用的不多

   TCC补偿方案 
即“局部通过方案”，要求部分事务系统准备好处理事务即可，相对比XA方案灵活了许多，同时它的处理方式是将事务问题交给系统本身处理，需要用大量的代码控制，优点是数据一致性也很高，缺点是控制事务的逻辑代码复杂冗余，性能也很差。所以这种方案也不常用。

   可靠消息最终一致性方案 
可靠消息最终一致性方案：这是一种目前市面上比较常用的方案，其原理与上述方法类似，需要借助MQ，只是不再借助数据库的消息表，而是由系统发起一条预发送消息，当系统本身的事务执行完毕后再将MQ中的消息变为确认消息，同样其他系统接收到MQ的消息后开始处理本地事务，根据处理情况决定事务是否需要回滚。相对来说优点是事务控制较为灵活，缺点是不稳定因素较多

##  分布式ID生成 
  UUID 
缺点：
不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。
信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，暴露使用者的位置。
对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能

  Redis自增原子命令 
   incr自增方法 
Redis 实现分布式全局唯一ID，它的性能比较高，生成的数据是有序的，对排序业务有利，但是同样它依赖于redis，需要系统引进redis组件，增加了系统的配置复杂性。

当然现在Redis的使用性很普遍，所以如果其他业务已经引进了Redis集群，则可以资源利用考虑使用Redis来实现。

  雪花算法Snowflake 
雪花算法提供了一个很好的设计思想，雪花算法生成的ID是趋势递增，不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的，而且可以根据自身业务特性分配bit位，非常灵活。

但是雪花算法强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。如果恰巧回退前生成过一些ID，而时间回退后，生成的ID就有可能重复
